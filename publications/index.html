<!DOCTYPE html>
<html lang="en, cn">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>publications | Anmin  Yang</title>
    <meta name="author" content="Anmin  Yang">
    <meta name="description" content="Anmin Yang's personal website. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="anmin-yang">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/my_avatar.JPG">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://anmin-yang.github.io/publications/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-lighter">Anmin </span>Yang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blogs</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">

<p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=aEcQu2EAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a>.</p>

<p>* denotes equal contribution.</p>
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Working</abbr></div>

  <!-- Entry bib key -->
  <div id="yang2023attention" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Shared and distinct neural signatures of feature and spatial attention</div>
    <!-- Author -->
    <div class="author">
      

      <em>Anmin Yang</em>, Liqin Zhou, Jinhua Tian, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Ke Zhou, Jia Liu' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em></em> 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The debate regarding whether feature attention (FA) and spatial attention (SA) share a common neural mechanism continues to persist. Previous studies have identified frontoparietal regions consistently activated during different visual attention tasks. However, these studies had limited sample sizes and methodological constraints inherent in univariate analysis. To address these limitations, the current study employed a machine-learning-based multivariate analysis approach, leveraging a large sample size of 235 participants. This enabled a comprehensive investigation into the shared and distinct neural signatures associated with FA and SA. Our findings revealed that both neural signatures exhibited the ability to predict each other, although inter-task prediction performance was notably weaker compared to intra-task prediction. The findings lends support to the presence of both shared and distinct neural mechanisms underlying FA and SA. Notably, the frontoparietal network emerged as the most predictive network for FA, while the visual network played a primary role in SA. Moreover, we observed overlapping areas between the two attention tasks. Specifically, through cluster-level analysis with single-cluster prediction, we identified four regions located at the temporal/occipital lobes (i.e., the left Lingual Gyrus (LG), right Occipital Pole (OcP), right Occipital Fusiform Gyrus (OFG), and left Temporal Occipital Fusiform Cortex (TOF)) that exhibited similar patterns on both SA and FA. Further analysis with virtual lesion techniques revealed that no single cluster was indispensable for predicting either FA or SA, suggesting a distributed neural network responsible for supporting both attention tasks. Additional voxel-level analysis comparing the neural signature patterns further corroborated the distributed nature of FA and SA. In sum, by utilizing a machinelearning-based multivariate analysis approach with a large sample size, our study provides comprehensive evidence regarding the shared and distinct neural mechanisms underlying FA and SA. This approach overcomes previous limitations and sheds new light on the intricate nature of attentional processes.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div>

  <!-- Entry bib key -->
  <div id="yang2023nlp" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Natural language processing of narrative writing for depression screening in adolescents</div>
    <!-- Author -->
    <div class="author">
      

      Tian Li*, <em>Anmin Yang*</em>, Guiting Zhang, and
      <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Xin Tang, Ke Zhou, Jia Liu' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Psyaxiv</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://psyarxiv.com/5f9nb/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://github.com/Anmin-Yang/nlp_depression" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Depression has become increasingly common among adolescents worldwide and has severe negative impact on an individual’s mental and social development. Early identification and diagnosis of depression during adolescence are crucial for improving the well-being of affected individuals. While natural language processing (NLP) has shown effectiveness in identifying depression in adults via social media, its application in predicting depression in adolescents remains challenging. This study aimed to develop a simple yet highly applicable method for predicting depression in adolescents within a school setting. We collected written compositions from 4,715 students aged 10 to 17, using their scores on the Children’s Depression Inventory (CDI) to categorize them into high-risk and low-risk groups for depression. Then, we developed three types of computational models combining various feature extraction (theory-based vs. data-based) and classification techniques (classical machine learning algorithm vs. state-of-the-art deep neural networks), and compared their predictive performance in identifying individuals at risk. We found that all models exhibited promising performance in predicting depressive tendencies, with the recurrent neural network model outperforming the others. Our study demonstrates the feasibility of employing students’ written compositions to identify those at higher risk of depression, and providing a potential solution for early detection of depressive tendencies in adolescents.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Cereb. Cortex</abbr></div>

  <!-- Entry bib key -->
  <div id="zhang2023connectome" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">A connectome-based neuromarker of nonverbal number acuity and arithmetic skills</div>
    <!-- Author -->
    <div class="author">
      

      Dai Zhang, Liqin Zhou, <em>Anmin Yang</em>, and
      <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Shanshan Li, Chunqi Chang, Jia Liu, Ke Zhou' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Cerebral Cortex</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://academic.oup.com/cercor/article/33/3/881/6543489" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The approximate number system (ANS) is vital for survival and reproduction in animals and is crucial for constructing abstract mathematical abilities in humans. Most previous neuroimaging studies focused on identifying discrete brain regions responsible for the ANS and characterizing their functions in numerosity perception. However, a neuromarker to characterize an individual’s ANS acuity is lacking, especially one based on whole-brain functional connectivity (FC). Here, based on the resting-state functional magnetic resonance imaging (rs-fMRI) data obtained from a large sample, we identified a distributed brain network (i.e. a numerosity network) using a connectome-based predictive modeling (CPM) analysis. The summed FC strength within the numerosity network reliably predicted individual differences in ANS acuity regarding behavior, as measured using a nonsymbolic number-comparison task. Furthermore, in an independent dataset of the Human Connectome Project (HCP), we found that the summed FC strength within the numerosity network also specifically predicted individual differences in arithmetic skills, but not domain-general cognitive abilities. Therefore, our findings revealed that the identified numerosity network could serve as an applicable neuroimaging-based biomarker of nonverbal number acuity and arithmetic skills.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">PBB</abbr></div>

  <!-- Entry bib key -->
  <div id="zhang2022attention" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">The Influence of Cue Validity on Social Attention and Exogenous Attention</div>
    <!-- Author -->
    <div class="author">
      

      Guiting Zhang, <em>Anmin Yang</em>, Jialun Sun, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Liqin Zhou, Ke Zhou' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Progress in Biochemistry and Biophysics</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.pibb.ac.cn/pibbcn/article/abstract/20220036?st=article_issue" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Social cues such as eye gaze, head direction, and walking direction of biological motion are critical for human survival and social interaction. Since social and peripheral cues both have reflexive characteristics of attentional orientation, social attention is often regarded as one kind of exogenous attention. However, empirical evidence suggests that this explanation cannot fully account for all phenomena of social attention. Whether social attention and exogenous attention possess the same mechanism remains unclear. Here, we used a typical spatial cueing paradigm to systematically examine the effects of cue validity on social attention and exogenous attention, triggered by eye gaze and peripheral cues, respectively. The results showed that both kinds of attention were affected by cue validity. With the increase of cue validity, the attention effects of eye gaze and peripheral cues increased. When the cue validity was noninformative (0.5) or strongly predictive (0.8), there was no significant difference in the attentional effects between social attention and exogenous attention. More importantly, however, when the cue validity was 0.2 (i. e., counterpredictive), the attentional effects of both cues were significantly different. While the facilitation effects of the eye gaze cue were weakened, the attentional effects of the peripheral cue were reversed and showed an inhibition pattern, suggesting that gaze-triggered attention is more strongly reflexive than exogenous attention orienting. Our finding thus provides new evidence supporting the theoretical hypothesis that there exist significant differences between social attention and classical exogenous attention, at least in certain stages of their processing. Our study also offers a new method to distinguish social attention and exogenous attention through voluntary attentional control.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Sci. Adv.</abbr></div>

  <!-- Entry bib key -->
  <div id="zhou2022emerged" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Emerged human-like facial expression representation in a deep convolutional neural network</div>
    <!-- Author -->
    <div class="author">
      

      Liqin Zhou, <em>Anmin Yang</em>, Ming Meng, and
      <span class="more-authors" title="click to view 1 more author" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '1 more author' ? 'Ke Zhou' : '1 more author';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">1 more author</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Science advances</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.science.org/doi/full/10.1126/sciadv.abj4383" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Recent studies found that the deep convolutional neural networks (DCNNs) trained to recognize facial identities
spontaneously learned features that support facial expression recognition, and vice versa. Here, we showed that
the self-emerged expression-selective units in a VGG-Face trained for facial identification were tuned to distinct
basic expressions and, importantly, exhibited hallmarks of human expression recognition (i.e., facial expression
confusion and categorical perception). We then investigated whether the emergence of expression-selective units
is attributed to either face-specific experience or domain-general processing by conducting the same analysis on
a VGG-16 trained for object classification and an untrained VGG-Face without any visual experience, both having
the identical architecture with the pretrained VGG-Face. Although similar expression-selective units were found
in both DCNNs, they did not exhibit reliable human-like characteristics of facial expression perception. Together,
these findings revealed the necessity of domain-specific visual experience of face identity for the development
of facial expression perception, highlighting the contribution of nurture to form human-like facial expression perception.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Sci. Data</abbr></div>

  <!-- Entry bib key -->
  <div id="liu2019manually" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">A manually denoised audio-visual movie watching fMRI dataset for the studyforrest project</div>
    <!-- Author -->
    <div class="author">
      

      Xingyu Liu, Zonglei Zhen, <em>Anmin Yang</em>, and
      <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Haohao Bai, Jia Liu' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Scientific Data</em>, 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.nature.com/articles/s41597-019-0303-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
      <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
      <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The data presented here are related to the studyforrest project that uses the movie ‘Forrest Gump’ to map brain functions in a real-life context using functional magnetic resonance imaging (fMRI). However, neural-related fMRI signals are often small and confounded by various noise sources (i.e., artifacts) that makes searching for the signals induced by specific cognitive processes significantly challenging. To make neural-related signals stand out from the noise, the audio-visual movie watching fMRI dataset from the project was denoised by a combination of spatial independent component analysis and manual identification of signals or noise. Here, both the denoised data and the labeled decomposed components are shared to facilitate further study. Compared with the original data, the denoised data showed a substantial improvement in the temporal signal-to-noise ratio and provided a higher sensitivity in subsequent analyses such as in an inter-subject correlation analysis.</p>
    </div>
  </div>
</div>
</li></ol>


</div>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Anmin  Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
 

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9NJECFJWJ2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-9NJECFJWJ2');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
